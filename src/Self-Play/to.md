
# Evaluation
livecodebench
lm_eval
MBPP

#SFT
How to sft reasoning model
sft w/o reasoning, is there a better lora adapter method? 
lora size compared to 1.5B? 

#SHQ
remove check compile
Use the SFT adapter and then finetune on it 
batch size experiment
lr experiment



utilize gpu for multi gen in multi gpus

eval_instruct result/adapter_path
