
utilize gpu for multi gen in multi gpus

last layer finetuning instead of LoRA

How to finetune reasoning model

pip install flashinfer-python